{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use SimSum Classification to Link FEBRL People Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/rachhouse/intro-to-data-linking/blob/main/tutorial_notebooks/01_Link_FEBRL_Data_with_SimSum_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll link synthesized people datasets generated by the [Freely Extensible Biomedical Record Linkage (FEBRL)](https://sourceforge.net/projects/febrl/) project. The FEBRL-generated datasets represent cleaned datasets, so in this notebook, we will step through:\n",
    "* data augmentation,\n",
    "* blocking,\n",
    "* comparing, and\n",
    "* classification using the SimSum methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we're running locally, or in Google Colab.\n",
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    COLAB = False\n",
    "    \n",
    "# If we're running in Colab, download the tutorial functions file \n",
    "# to the Colab session local directory, and install required libraries.\n",
    "if COLAB:\n",
    "    import requests\n",
    "    \n",
    "    tutorial_functions_url = \"https://raw.githubusercontent.com/rachhouse/intro-to-data-linking/main/tutorial_notebooks/linking_tutorial_functions.py\"\n",
    "    r = requests.get(tutorial_functions_url)\n",
    "    \n",
    "    with open(\"linking_tutorial_functions.py\", \"w\") as fh:\n",
    "        fh.write(r.text)\n",
    "    \n",
    "    !pip install -q recordlinkage jellyfish altair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "import altair as alt\n",
    "import jellyfish\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import recordlinkage as rl\n",
    "\n",
    "# We have a couple helper functions from this file that we'll use for evaluation.\n",
    "import linking_tutorial_functions as tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's set up access to a few data resources that we'll need for the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATASET_A, TRAINING_DATASET_B, TRAINING_LABELS = tutorial.get_training_data_paths(COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load (Cleaned) Training Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load our training datasets into pandas DataFrames. We want to be able to take advantage of pandas indexing as we link our data (plus, the `recordlinkage` package that we'll be using later needs input DataFrames to be indexed by record id), so we'll set an index on each training DataFrame.\n",
    "\n",
    "As mentioned above, we can consider the cleaning step of linking to be already done - the data generated by FEBRL is in a consistent format, and equivalent attributes have been encoded in the same manner for the two synthesized people datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A = pd.read_csv(TRAINING_DATASET_A)\n",
    "df_A = df_A.set_index(\"person_id_A\")\n",
    "df_A.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_B = pd.read_csv(TRAINING_DATASET_B)\n",
    "df_B = df_B.set_index(\"person_id_B\")\n",
    "df_B.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Ground Truth Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advantages of synthesized data, especially for tutorials and learning, is that we have ground truth labels for data. (This is rarely the case when you encounter linking problems in the wild). We'll load our known true links into a pandas DataFrame below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ground_truth = pd.read_csv(TRAINING_LABELS)\n",
    "df_ground_truth = df_ground_truth.set_index([\"person_id_A\", \"person_id_B\"])\n",
    "df_ground_truth[\"ground_truth\"] = df_ground_truth[\"ground_truth\"].apply(lambda x: True if x == 1 else False)\n",
    "df_ground_truth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our data, and consider what we have currently available for blocking and comparing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would probably make sense to block on people's first and last name, but, as we've noted, the realities of data entry typos, nicknames, aliases, OCR mishaps, and speech-to-text blips mean that using an exact blocker isn't going to work well. These fields are prime candidates for phonetic encoding!\n",
    "\n",
    "We'll use the python [jellyfish library](https://pypi.org/project/jellyfish/) to encode our `first_name` and `surname` fields via two phonetic encoding algorithms, [**Soundex**](https://en.wikipedia.org/wiki/Soundex) and [**NYSIIS**](https://en.wikipedia.org/wiki/New_York_State_Identification_and_Intelligence_System).\n",
    "\n",
    "We could also use a truncated exact blocking approach with the `soc_sec_id` field. For this, we'll create a new attribute containing the last three digits of the SSid.\n",
    "\n",
    "And lastly, we'll cast the `date_of_birth` field to a pandas Timestamp field so that we can compare it more easily down the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dob_to_date(dob: str) -> Optional[pd.Timestamp]:\n",
    "    \"\"\" Transform string date in YYYYMMDD format to a pd.Timestamp.\n",
    "        Return None if transformation is not successful.\n",
    "    \"\"\"\n",
    "    date_pattern = r\"(\\d{4})(\\d{2})(\\d{2})\"\n",
    "    dob_timestamp = None\n",
    "    \n",
    "    try:\n",
    "        m = re.match(date_pattern, dob.strip())\n",
    "        if m:\n",
    "            dob_timestamp = pd.Timestamp(int(m.group(1)), int(m.group(2)), int(m.group(3)))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return dob_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for df in [df_A, df_B]:\n",
    "    \n",
    "    # Update NaNs to empty strings or jellyfish will choke.\n",
    "    df[\"surname\"] = df[\"surname\"].fillna(\"\")\n",
    "    df[\"first_name\"] = df[\"first_name\"].fillna(\"\")\n",
    "\n",
    "    # Soundex phonetic encodings.\n",
    "    df[\"soundex_surname\"] = df[\"surname\"].apply(lambda x: jellyfish.soundex(x))\n",
    "    df[\"soundex_firstname\"] = df[\"first_name\"].apply(lambda x: jellyfish.soundex(x))\n",
    "    \n",
    "    # NYSIIS phonetic encodings.    \n",
    "    df[\"nysiis_surname\"] = df[\"surname\"].apply(lambda x: jellyfish.nysiis(x))\n",
    "    df[\"nysiis_firstname\"] = df[\"first_name\"].apply(lambda x: jellyfish.nysiis(x))\n",
    "    \n",
    "    # Last 3 of SSID.\n",
    "    df[\"ssid_last3\"] = df[\"soc_sec_id\"].apply(lambda x: str(x)[-3:].zfill(3) if x else None)\n",
    "    df[\"soc_sec_id\"] = df[\"soc_sec_id\"].astype(str)\n",
    "    \n",
    "    # DOB to date object.\n",
    "    df[\"dob\"] = df[\"date_of_birth\"].apply(lambda x: dob_to_date(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a sample of our new columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've augmented our datasets, let's try some blocking! We'll use the python [`recordlinkage` library](https://github.com/J535D165/recordlinkage) for blocking. \n",
    "\n",
    "First, let's see how many candidate record pairs we would generate with a full blocker - meaning if we compared every record in dataset A to every record in dataset B. This produces the [Cartesian product](https://en.wikipedia.org/wiki/Cartesian_product) of the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = rl.Index()\n",
    "indexer.add(rl.index.Full())\n",
    "\n",
    "full_blocker_pairs = indexer.index(df_A, df_B)\n",
    "max_candidate_record_pairs = full_blocker_pairs.shape[0]\n",
    "\n",
    "print(\"\\ndataset A size * dataset B size = maximum candidate record pairs\")\n",
    "print(f\"{df_A.shape[0]:,} * {df_B.shape[0]:,} = {df_A.shape[0]*df_B.shape[0]:,}\")\n",
    "\n",
    "print(f\"\\n{max_candidate_record_pairs:,} total pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`indexer.index` returns a pandas MultiIndex of the candidate record pairs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_blocker_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for very small datasets, like our training data, we're looking a huge amount of candidate record pairs to compare, unless we employ more selective blocking.\n",
    "\n",
    "Recall that successful and efficient blocking minimizes:\n",
    "* the quantity of generated candidate record pairs\n",
    "* missed true links\n",
    "\n",
    "So, first let's define a method which measures the percentage of true links captured by blocking, as well as the search space reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_blocking(\n",
    "    candidate_pairs: pd.MultiIndex,\n",
    "    df_left: pd.DataFrame,\n",
    "    df_right: pd.DataFrame,\n",
    "    df_true_links: pd.DataFrame\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\" Function to calculate blocking search space reduction and retained true links.\n",
    "        Reports and returns search space reduction percentage and retained true links percentage.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate search space reduction.\n",
    "    search_space_reduction = round(rl.reduction_ratio(candidate_pairs.shape[0], df_left, df_right), 3)\n",
    "    \n",
    "    # Calculate retained true links percentage.\n",
    "    total_true_links = df_true_links.shape[0]\n",
    "    true_links_after_blocking = pd.merge(\n",
    "        df_true_links,\n",
    "        candidate_pairs.to_frame(),\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how=\"inner\"\n",
    "    ).shape[0]\n",
    "    \n",
    "    retained_true_link_percent = round((true_links_after_blocking/total_true_links) * 100, 2)\n",
    "    \n",
    "    print(f\"{candidate_pairs.shape[0]:,} pairs after blocking: {search_space_reduction}% search space reduction.\")\n",
    "    print(f\"{retained_true_link_percent}% true links retained after blocking.\")\n",
    "    \n",
    "    return search_space_reduction, retained_true_link_percent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the full blocker as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_, _ = evaluate_blocking(full_blocker_pairs, df_A, df_B, df_ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense. If we use a full blocker, we won't have reduced our search space at all. And, since we consider every possible candidate pair, this will include all true links.\n",
    "\n",
    "However, let's see if we can do better. Let's experiment with a few sets of different blockers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = rl.Index()\n",
    "\n",
    "indexer.add(rl.index.Block(\"surname\"))\n",
    "\n",
    "candidate_pairs = indexer.index(df_A, df_B)\n",
    "\n",
    "_, _ = evaluate_blocking(candidate_pairs, df_A, df_B, df_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = rl.Index()\n",
    "\n",
    "indexer.add(rl.index.Block(\"surname\"))\n",
    "indexer.add(rl.index.Block(\"first_name\"))\n",
    "\n",
    "candidate_pairs = indexer.index(df_A, df_B)\n",
    "\n",
    "_, _ = evaluate_blocking(candidate_pairs, df_A, df_B, df_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = rl.Index()\n",
    "\n",
    "indexer.add(rl.index.Block(\"soundex_surname\"))\n",
    "indexer.add(rl.index.Block(\"soundex_firstname\"))\n",
    "indexer.add(rl.index.Block(\"nysiis_surname\"))\n",
    "indexer.add(rl.index.Block(\"nysiis_firstname\"))\n",
    "\n",
    "candidate_pairs = indexer.index(df_A, df_B)\n",
    "\n",
    "_, _ = evaluate_blocking(candidate_pairs, df_A, df_B, df_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = rl.Index()\n",
    "\n",
    "indexer.add(rl.index.Block(\"soundex_surname\"))\n",
    "indexer.add(rl.index.Block(\"soundex_firstname\"))\n",
    "indexer.add(rl.index.Block(\"nysiis_surname\"))\n",
    "indexer.add(rl.index.Block(\"nysiis_firstname\"))\n",
    "indexer.add(rl.index.Block(\"ssid_last3\"))\n",
    "indexer.add(rl.index.Block(\"date_of_birth\"))\n",
    "\n",
    "candidate_pairs = indexer.index(df_A, df_B)\n",
    "\n",
    "_, _ = evaluate_blocking(candidate_pairs, df_A, df_B, df_ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we're reasonably satisifed with our blockers, we can move on to comparing our candidate record pairs. Recall that in the comparison step, for each candidate record pair, we compare their attributes to generate a comparison vector. Once again, we'll use [`recordlinkage`](https://github.com/J535D165/recordlinkage) to define our comparators. `recordlinkage` offers a variety of built-in comparators to use for string, numeric, and datetime fields.\n",
    "\n",
    "* We can use exact comparators for our phonetic encoding fields.\n",
    "* We'll use Jaro-Winkler comparison for the name fields, as this comparison approach is specifically designed for comparison of names.\n",
    "* For the other string fields, we'll opt for Damerau-Levenshtein, which does a nice job in accomodating data entry typos.\n",
    "* For the DOB, we'll use a date comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "comparer = rl.Compare()\n",
    "\n",
    "# Phonetic encodings.\n",
    "comparer.add(rl.compare.Exact(\"soundex_surname\", \"soundex_surname\", label=\"soundex_surname\"))\n",
    "comparer.add(rl.compare.Exact(\"soundex_firstname\", \"soundex_firstname\", label=\"soundex_firstname\"))\n",
    "comparer.add(rl.compare.Exact(\"nysiis_surname\", \"nysiis_surname\", label=\"nysiis_surname\"))\n",
    "comparer.add(rl.compare.Exact(\"nysiis_firstname\", \"nysiis_firstname\", label=\"nysiis_firstname\"))\n",
    "\n",
    "# First & last name.\n",
    "comparer.add(rl.compare.String(\"surname\", \"surname\", method=\"jarowinkler\", label=\"last_name\"))\n",
    "comparer.add(rl.compare.String(\"first_name\", \"first_name\", method=\"jarowinkler\", label=\"first_name\"))\n",
    "\n",
    "# Address.\n",
    "comparer.add(rl.compare.String(\"address_1\", \"address_1\", method=\"damerau_levenshtein\", label=\"address_1\"))\n",
    "comparer.add(rl.compare.String(\"address_2\", \"address_2\", method=\"damerau_levenshtein\", label=\"address_2\"))\n",
    "comparer.add(rl.compare.String(\"suburb\", \"suburb\", method=\"damerau_levenshtein\", label=\"suburb\"))\n",
    "comparer.add(rl.compare.String(\"postcode\", \"postcode\", method=\"damerau_levenshtein\", label=\"postcode\"))\n",
    "comparer.add(rl.compare.String(\"state\", \"state\", method=\"damerau_levenshtein\", label=\"state\"))\n",
    "\n",
    "# Other fields.\n",
    "comparer.add(rl.compare.Date(\"dob\", \"dob\", label=\"date_of_birth\"))\n",
    "comparer.add(rl.compare.String(\"phone_number\", \"phone_number\", method=\"damerau_levenshtein\", label=\"phone_number\"))\n",
    "comparer.add(rl.compare.String(\"soc_sec_id\", \"soc_sec_id\", method=\"damerau_levenshtein\", label=\"ssn\"))\n",
    "\n",
    "features = comparer.compute(candidate_pairs, df_A, df_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the output of the compare step is a collection of comparison/feature vectors, one for each candidate record pair. `recordlinkage` returns these vectors as a pandas Dataframe, indexed on the record pair ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a look at an individual comparison vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(features.iloc[0].name)\n",
    "display(features.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add labels to feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've generated our comparison/feature vectors, now we're ready to classify! To begin, we'll add our ground truth labels to the features DataFrame. Note that `df_ground_truth` just contains the true links, so we'll use a left join and then `fillna` with `False` for any records that are not true links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled_features = pd.merge(\n",
    "    features,\n",
    "    df_ground_truth,\n",
    "    on=[\"person_id_A\", \"person_id_B\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "df_labeled_features[\"ground_truth\"].fillna(False, inplace=True)\n",
    "df_labeled_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate SimSum Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, SimSum is the simplest approach to linking classification. To generate our scores for the candidate record pairs, we simply sum the values each attribute comparison score into a single score for each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled_features[\"simsum\"] = df_labeled_features.drop(\"ground_truth\", axis=1).sum(axis=1)\n",
    "df_labeled_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a SimSum Classification Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've generated scores for all of our candidate record pairs, the next step is to determine a threshold at which we can classify a record pair as a link, or not-a-link. To do this, it's first helpful to look at the score distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Model\" Score Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a pretty clear boundary between not-links and links when it comes to the SimSum score. There's a bit of an overlap from 7 - 9.5, but it looks like we'll probably want to set the cutoff somewhere in that range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial.plot_model_score_distribution(\n",
    "    df_labeled_features,\n",
    "    score_column_name=\"simsum\",  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall at Varying Thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll take a look at the calculated precision and recall at varying model score thresholds. Below is a function which calculates precision and recall for a range of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_linking(\n",
    "    df: pd.DataFrame,\n",
    "    score_column_name: Optional[str] = \"score\",\n",
    "    ground_truth_column_name: Optional[str] = \"ground_truth\",\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\" Use model results to calculate precision & recall metrics.\n",
    "    \n",
    "        Args:\n",
    "            df: dataframe containing model scores, and ground truth labels\n",
    "                indexed on df_left index, df_right index\n",
    "            score_column_name: Optional string name of column containing model scores \n",
    "            ground_truth_column_name: Optional string name of column containing ground\n",
    "                truth values\n",
    "                \n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "                pandas dataframe with precision and recall evaluation data\n",
    "                at varying score thresholds\n",
    "    \"\"\"\n",
    "    eval_data = []\n",
    "    max_score = max(1, max(df[score_column_name]))\n",
    "    \n",
    "    # Calculate eval data at threshold intervals from zero to max score. \n",
    "    # Max score is generally 1.0 if using a ML model, but with SimSum it\n",
    "    # can get much larger.\n",
    "    for threshold in np.linspace(0, max_score, 50):\n",
    "        tp = df[(df[score_column_name] >= threshold) & (df[ground_truth_column_name] == True)].shape[0]\n",
    "        fp = df[(df[score_column_name] >= threshold) & (df[ground_truth_column_name] == False)].shape[0]\n",
    "        tn = df[(df[score_column_name] < threshold) & (df[ground_truth_column_name] == False)].shape[0]\n",
    "        fn = df[(df[score_column_name] < threshold) & (df[ground_truth_column_name] == True)].shape[0]\n",
    "        \n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = 2 * ((precision * recall)/(precision + recall))\n",
    "        \n",
    "        eval_data.append(\n",
    "            {\n",
    "                \"threshold\" : threshold,\n",
    "                \"tp\" : tp,\n",
    "                \"fp\" : fp,\n",
    "                \"tn\" : tn,\n",
    "                \"fn\" : fn,\n",
    "                \"precision\" : precision,\n",
    "                \"recall\" : recall,\n",
    "                \"f1\" : f1\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    return pd.DataFrame(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = evaluate_linking(\n",
    "    df=df_labeled_features,\n",
    "    score_column_name = \"simsum\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot precision and recall at varying score thresholds reinforces what we noted earlier in the score distribution - that our most suitable cutoff is in the range of 7 to 9.5. It relies on your own particular use case to determine exactly where the cutoff should be set (e.g. Is recall more important than precision, or vice versa?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial.plot_precision_recall_vs_threshold(df_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the F1 score at varying model thresholds. F1 is the harmonic mean of precision and recall, which provides us with a single figure to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial.plot_f1_score_vs_threshold(df_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Individual Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to gain insight into the performance of link classification is examining individual links (including their original attribute values) in score ranges of interest. This can be particularly helpful where you see overlap of classes in your score distribution - i.e. where you see highly scored non-links and poorly scored true links. These cases can highlight model confusion, and shed light on potential feature improvements.\n",
    "\n",
    "Below, we've:\n",
    "* Defined a helper function to join scored pairs with their original entity attribute data\n",
    "* Captured the top scoring non-links (negatives) as well as the lowest scoring true links (positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_scored_pairs(\n",
    "    df: pd.DataFrame,\n",
    "    df_left: pd.DataFrame,\n",
    "    df_right: pd.DataFrame,\n",
    "    score_column_name: Optional[str] = \"score\",\n",
    "    ground_truth_column_name: Optional[str] = \"ground_truth\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\" Augment scored pairs with original entity attribute data.\n",
    "    \n",
    "        Args:\n",
    "            df: dataframe containing pairs for examination that includes\n",
    "                model scores and ground truth labels, and is indexed on\n",
    "                df_left index, df_right index\n",
    "            df_left: dataframe containing attributes for \"left\"-linked entities\n",
    "            df_right: dataframe containing attributes for \"right\"-linked entities\n",
    "            score_column_name: Optional string name of column containing model scores \n",
    "            ground_truth_column_name: Optional string name of column containing ground\n",
    "                truth values\n",
    "                \n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "                pandas dataframe containing pairs augmented with original entity attributes \n",
    "    \"\"\"\n",
    "    \n",
    "    df = df[[score_column_name, ground_truth_column_name]]\n",
    "\n",
    "    # Suffix our original attribute fields for display convenience when\n",
    "    # we examine the links in the notebook.\n",
    "    df_left = df_left.copy()\n",
    "    df_left.columns = df_left.columns.map(lambda x: str(x) + '_A')\n",
    "\n",
    "    df_right = df_right.copy()\n",
    "    df_right.columns = df_right.columns.map(lambda x: str(x) + '_B')\n",
    "    \n",
    "    # Join the original link entity data via the dataframe indices.\n",
    "    # This gives us the model score as well as the actual human-readable attributes\n",
    "    # for each link.\n",
    "    df_augmented_pairs = pd.merge(\n",
    "        df,\n",
    "        df_left,\n",
    "        left_on=df_left.index.name,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # Join data from right entities.\n",
    "    df_augmented_pairs = pd.merge(\n",
    "        df_augmented_pairs,\n",
    "        df_right,\n",
    "        left_on=df_right.index.name,\n",
    "        right_index=True,\n",
    "    ) \n",
    "    \n",
    "    return df_augmented_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_cols = [\n",
    "    \"first_name\", \"surname\",\n",
    "    \"street_number\", \"address_1\", \"address_2\", \"suburb\", \"postcode\", \"state\",\n",
    "    \"date_of_birth\", \"age\", \"phone_number\", \"soc_sec_id\",\n",
    "    \"soundex_surname\", \"soundex_firstname\",\n",
    "    \"nysiis_surname\", \"nysiis_firstname\",\n",
    "]\n",
    "\n",
    "display_cols = [[f\"{col}_A\", f\"{col}_B\"] for col in display_cols]\n",
    "display_cols = list(itertools.chain.from_iterable(display_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Scoring Non-Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_scoring_negatives = df_labeled_features[\n",
    "    df_labeled_features[\"ground_truth\"] == False\n",
    "][[\"simsum\", \"ground_truth\"]].sort_values(\"simsum\", ascending=False).head(n=10)\n",
    "\n",
    "df_top_scoring_negatives = augment_scored_pairs(df_top_scoring_negatives, df_A, df_B, score_column_name=\"simsum\")\n",
    "\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_top_scoring_negatives[[\"simsum\", \"ground_truth\"] + display_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowest Scoring True Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lowest_scoring_positives = df_labeled_features[\n",
    "    df_labeled_features[\"ground_truth\"] == True\n",
    "][[\"simsum\", \"ground_truth\"]].sort_values(\"simsum\").head(n=10)\n",
    "\n",
    "df_lowest_scoring_positives = augment_scored_pairs(df_lowest_scoring_positives, df_A, df_B, score_column_name=\"simsum\")\n",
    "\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_lowest_scoring_positives[[\"simsum\", \"ground_truth\"] + display_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linking",
   "language": "python",
   "name": "linking"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
